{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import zeus.notebook_utils.syspath as syspath\n",
    "syspath.add_parent_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from zeus.core.random import super_seed\n",
    "from zeus.torch_tools.checkpoints import find_latest_dir\n",
    "from zeus.utils import TimestampFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kidney.datasets.kaggle import get_reader\n",
    "from kidney.datasets.offline import create_data_loaders\n",
    "from kidney.datasets.utils import read_segmentation_info\n",
    "from kidney.experiments.smp import SMPExperiment, parse_fold_keys\n",
    "from kidney.utils.checkpoints import CheckpointsStorage, load_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import Any, Dict, Type\n",
    "from zeus.utils import named_match\n",
    "\n",
    "def parse_string(filename: str, patterns: Dict) -> Dict[str, Any]:\n",
    "    entries = []\n",
    "    for name, template in patterns.items():\n",
    "        if template is int or template is float:\n",
    "            template = {int: r\"\\d+\", float: r\"\\d+.\\d+\"}[template]\n",
    "        entries.append(fr\"{name}=(?P<{name}>{template})\")\n",
    "    regex = \"_\".join(entries)\n",
    "    return named_match(pattern=regex, string=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    parse_string(\"epoch=14_avg_val_loss=0.0403.ckpt\", {'epoch': r'\\d+', 'avg_val_loss': r'\\d+.\\d+'}) ==\n",
    "    {'epoch': 14, 'avg_val_loss': 0.0403}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing metrics on checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS = \"/home/ck/experiments/smp/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = get_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = CheckpointsStorage(CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = storage.fetch_available_checkpoints(\"avg_val_loss\", best_checkpoint_per_date=False)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files, meta_file = benchmark[\"checkpoints\"], benchmark[\"meta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(checkpoint_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted_files = [\n",
    "    filename\n",
    "    for filename, _ in \n",
    "    sorted([\n",
    "        (fn, parse_string(fn, {\"epoch\": int})[\"epoch\"]) \n",
    "        for fn in checkpoint_files\n",
    "    ], key=itemgetter(1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_files[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_checkpoint = []\n",
    "\n",
    "for i, checkpoint_file in enumerate(sorted_files):\n",
    "    print(f\"[{i+1:3d}/{total:3d}] inference: {checkpoint_file}\")\n",
    "    \n",
    "    experiment, meta = load_experiment(SMPExperiment, checkpoint_file, meta_file)\n",
    "    \n",
    "    experiment.to(device)\n",
    "    params = meta[\"params\"]\n",
    "    super_seed(params.seed)\n",
    "    \n",
    "    loaders = create_data_loaders(\n",
    "        reader=reader,\n",
    "        valid_keys=parse_fold_keys(params.fold) if params.fold is not None else params.fold,\n",
    "        transformers=meta[\"transformers\"],\n",
    "        samples=read_segmentation_info(params.dataset, file_format=params.file_format),\n",
    "        num_workers=params.num_workers,\n",
    "        batch_size=params.batch_size,\n",
    "        multiprocessing_context=params.data_loader_multiprocessing_context\n",
    "    )\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        metrics = {\"train\": [], \"valid\": []}\n",
    "        for name, loader in loaders.items():\n",
    "            for batch in loader:\n",
    "                batch = {key: tensor.to(device) for key, tensor in batch.items()}\n",
    "                outputs = experiment(batch)\n",
    "                batch_metrics = {\n",
    "                    metric.name.replace(' ', '_'): metric(outputs, batch).item() \n",
    "                    for metric in experiment.metrics\n",
    "                }\n",
    "                metrics[name].append(batch_metrics)\n",
    "                \n",
    "    metrics_per_checkpoint.append({\n",
    "        \"order\": i, \n",
    "        \"filename\": checkpoint_file, \n",
    "        \"batch_metrics\": metrics\n",
    "    })\n",
    "            \n",
    "    del experiment, loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(metrics_per_checkpoint, \"/home/ck/benchmark.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from zeus.plotting.style import notebook_style\n",
    "from zeus.plotting.utils import axes, calculate_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_style(override={'axes.grid': True, 'figure.figsize': (12, 8)}).apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = torch.load(\"/home/ck/benchmark.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for checkpoint in benchmark:\n",
    "    metrics = checkpoint[\"batch_metrics\"]\n",
    "    from_file = parse_string(checkpoint[\"filename\"], {\"avg_val_loss\": float})\n",
    "    record = OrderedDict()\n",
    "    record[\"epoch\"] = checkpoint[\"order\"]\n",
    "    record[\"avg_val_loss\"] = from_file[\"avg_val_loss\"]\n",
    "    for subset in metrics.keys():\n",
    "        collected = defaultdict(list)\n",
    "        for batch in metrics[subset]:\n",
    "            for metric, value in batch.items():\n",
    "                collected[metric].append(value)\n",
    "        avg, std = {}, {}\n",
    "        for name, values in collected.items():\n",
    "            avg[name], std[name] = np.mean(values), np.std(values)        \n",
    "        record.update([(f\"{subset}_mean_{k}\", v) for k, v in avg.items()])\n",
    "        record.update([(f\"{subset}_std_{k}\", v) for k, v in std.items()])\n",
    "    table.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"train_mean_\", \"valid_mean_\"]\n",
    "wide = pd.wide_to_long(table, prefixes, i=\"epoch\", j=\"metric\", suffix=\"\\w+\")\n",
    "wide = wide[prefixes].rename(columns=dict(zip(prefixes, [\"train\", \"valid\"])))\n",
    "wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metric, ax=None):\n",
    "    ax = axes(ax=ax)\n",
    "    ax = wide.xs(metric, level=1).plot(ax=ax)\n",
    "    ax.set_title(metric.title())\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {name for _, name in wide.index}\n",
    "\n",
    "_, axs = plt.subplots(*calculate_layout(len(metric_names)), figsize=(30, 20))\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.axis(False)\n",
    "\n",
    "for ax, metric in zip(axs.flat, metric_names):\n",
    "    plot(metric, ax=ax)\n",
    "    ax.axis(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.melt(wide.reset_index(), id_vars=[\"epoch\", \"metric\"], var_name=\"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_train = alt.Chart(metrics_df.query(\"subset == 'train'\")).mark_line().encode(x=\"epoch\", y=\"value\", color=\"metric\").properties(title=\"train\")\n",
    "chart_valid = alt.Chart(metrics_df.query(\"subset == 'valid'\")).mark_line().encode(x=\"epoch\", y=\"value\", color=\"metric\").properties(title=\"valid\")\n",
    "chart_train | chart_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in (\"dice\", \"balanced_accuracy\", \"recall\", \"precis\")\n",
    "wide.xs(\"dice\", level=1)[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = wide.xs(\"precision\", level=1)\n",
    "recall = wide.xs(\"recall\", level=1)\n",
    "\n",
    "f1_score = pd.DataFrame({\n",
    "    \"epoch\": precision.index,\n",
    "    \"metric\": [\"f1_score\"] * len(precision),\n",
    "})\n",
    "\n",
    "for subset in (\"train\", \"valid\"):\n",
    "    p, r = precision[subset], recall[subset] \n",
    "    f1 = 2*p*r/(p + r)\n",
    "    f1_score[subset] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([wide.reset_index(), f1_score]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = []\n",
    "for metric in (\"dice\", \"precision\", \"recall\", \"f1_score\", \"balanced_accuracy\"):\n",
    "    df = metrics_df.query(f\"metric == '{metric}'\")\n",
    "    best_index = df[\"valid\"].argmax()\n",
    "    record = df.iloc[best_index]\n",
    "    best.append({\"metric\": metric, \"epoch\": record.epoch, \"best\": record.valid})\n",
    "best = pd.DataFrame(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best[\"filename\"] = best.epoch.map(lambda epoch: sorted_files[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.filename.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kidney (3.7)",
   "language": "python",
   "name": "kidney"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
